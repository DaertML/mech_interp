Not in IPython
Imported everything!
tensor([[  0,   0, 113],
        [  0,   1, 113],
        [  0,   2, 113],
        ...,
        [112, 110, 113],
        [112, 111, 113],
        [112, 112, 113]], device='cuda:0')
Train loss: tensor(4.7354, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)
Test loss: tensor(4.7340, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)
Uniform loss: 4.727387818712341
Epoch 0 Train Loss 4.735448327362934 Test Loss 4.73364486838679
Epoch 1 Train Loss 4.726511494675351 Test Loss 4.735843021369546
Epoch 2 Train Loss 4.720987497336029 Test Loss 4.7392082807925044
Epoch 3 Train Loss 4.717292841121687 Test Loss 4.742919430240605
Epoch 4 Train Loss 4.714567744822775 Test Loss 4.74633808120548
Epoch 5 Train Loss 4.712265037372209 Test Loss 4.749146875143793
Epoch 6 Train Loss 4.710080200999863 Test Loss 4.751313711191453
Epoch 7 Train Loss 4.707866022475364 Test Loss 4.752949172359009
Epoch 8 Train Loss 4.705583147689282 Test Loss 4.754268527678618
Epoch 9 Train Loss 4.703278007194828 Test Loss 4.7554976964268345
Epoch 10 Train Loss 4.700950669642518 Test Loss 4.7567898111626254
Epoch 11 Train Loss 4.698576499572166 Test Loss 4.758165493011531
Epoch 12 Train Loss 4.696049769799362 Test Loss 4.759684490558093
Epoch 13 Train Loss 4.693325354480293 Test Loss 4.761336357614011
Epoch 14 Train Loss 4.690315929831107 Test Loss 4.763122297029932
Epoch 15 Train Loss 4.686968335353395 Test Loss 4.76506975570439
Epoch 16 Train Loss 4.683244807322846 Test Loss 4.767251906293302
Epoch 17 Train Loss 4.679086960274264 Test Loss 4.769798016358956
Epoch 18 Train Loss 4.674439759636582 Test Loss 4.772854074538605
Epoch 19 Train Loss 4.669255961899374 Test Loss 4.776570135869307
Epoch 20 Train Loss 4.66348121031329 Test Loss 4.781105118924044
Epoch 21 Train Loss 4.657063657728951 Test Loss 4.786669332126927
Epoch 22 Train Loss 4.6499314723108265 Test Loss 4.793499308474319
Epoch 23 Train Loss 4.642042695827625 Test Loss 4.801910616425142
Epoch 24 Train Loss 4.63340325505074 Test Loss 4.81220866247408
Epoch 25 Train Loss 4.624009999606863 Test Loss 4.824676918099078
Epoch 26 Train Loss 4.613898872589924 Test Loss 4.839590452354293
Epoch 27 Train Loss 4.603158754210148 Test Loss 4.8571705725813965
Epoch 28 Train Loss 4.591906132933315 Test Loss 4.877426994748538
Epoch 29 Train Loss 4.580203031228908 Test Loss 4.9002136767193925
Epoch 30 Train Loss 4.568128948928765 Test Loss 4.925207009859572
Epoch 31 Train Loss 4.555691659838763 Test Loss 4.951910136713681
Epoch 32 Train Loss 4.542890220386653 Test Loss 4.979781280903267
Epoch 33 Train Loss 4.529760438351925 Test Loss 5.008593749466478
Epoch 34 Train Loss 4.5163864240209595 Test Loss 5.0380956685756395
Epoch 35 Train Loss 4.502859961620749 Test Loss 5.067835284341666
Epoch 36 Train Loss 4.489289531612136 Test Loss 5.097149043895985
Epoch 37 Train Loss 4.475671386773236 Test Loss 5.125463413168935
Epoch 38 Train Loss 4.4619480396689895 Test Loss 5.152588750072536
Epoch 39 Train Loss 4.44802155539239 Test Loss 5.178530828631223
Epoch 40 Train Loss 4.433929121624998 Test Loss 5.203064903722417
Epoch 41 Train Loss 4.419775058176416 Test Loss 5.226334676459174
Epoch 42 Train Loss 4.405623801016012 Test Loss 5.248870056268016
Epoch 43 Train Loss 4.391442663190761 Test Loss 5.271061703376165
Epoch 44 Train Loss 4.377244866553886 Test Loss 5.293012105546374
Epoch 45 Train Loss 4.363022507285255 Test Loss 5.31517935625289
Epoch 46 Train Loss 4.348807142604059 Test Loss 5.33793467660645
Epoch 47 Train Loss 4.334617194549988 Test Loss 5.361075079372165
Epoch 48 Train Loss 4.320375031596168 Test Loss 5.384514114688116
Epoch 50 Train Loss 4.291169982184498 Test Loss 5.432784163426635
Epoch 51 Train Loss 4.276144691361156 Test Loss 5.457380752719657
Epoch 52 Train Loss 4.260959401568712 Test Loss 5.482898511329664
Epoch 53 Train Loss 4.245660659995685 Test Loss 5.509240950885101
Epoch 54 Train Loss 4.230157855518971 Test Loss 5.535845121984895
Epoch 55 Train Loss 4.21447077991296 Test Loss 5.562478647994511
Epoch 56 Train Loss 4.198692937546193 Test Loss 5.5892058353949565
Epoch 57 Train Loss 4.182759296058987 Test Loss 5.6159227761177375
Epoch 58 Train Loss 4.166575956811386 Test Loss 5.642523037090166
Epoch 59 Train Loss 4.150116733602915 Test Loss 5.669459448943807
Epoch 60 Train Loss 4.1333643735166055 Test Loss 5.697672535309174
Epoch 61 Train Loss 4.116243177195089 Test Loss 5.726357397407491
Epoch 62 Train Loss 4.098708470721085 Test Loss 5.755531323727381
Epoch 63 Train Loss 4.080758197930281 Test Loss 5.785696007385321
Epoch 64 Train Loss 4.062420549117453 Test Loss 5.8162628094477675
Epoch 65 Train Loss 4.043673626015702 Test Loss 5.84731608602603
Epoch 66 Train Loss 4.024572945494553 Test Loss 5.879627481822349
Epoch 67 Train Loss 4.005081968768174 Test Loss 5.912469459932768
Epoch 68 Train Loss 3.98513088750275 Test Loss 5.945389320542257
Epoch 69 Train Loss 3.9646826984784034 Test Loss 5.979044236391803
Epoch 70 Train Loss 3.9437416523277498 Test Loss 6.013346759886464
Epoch 71 Train Loss 3.922252701953363 Test Loss 6.047785122411746
Epoch 72 Train Loss 3.9002243392411624 Test Loss 6.082826634293514
Epoch 73 Train Loss 3.877506671276378 Test Loss 6.118127376777289
Epoch 74 Train Loss 3.8541020644496755 Test Loss 6.153794975176865
Epoch 75 Train Loss 3.8300223982383774 Test Loss 6.189514771067371
Epoch 76 Train Loss 3.805162596543063 Test Loss 6.224999907869455
Epoch 77 Train Loss 3.7795327063993835 Test Loss 6.2609851191789865
Epoch 78 Train Loss 3.7530836552200824 Test Loss 6.29793989831091
Epoch 79 Train Loss 3.7257673305591372 Test Loss 6.334199572327506
Epoch 80 Train Loss 3.6975700385105554 Test Loss 6.372859567744804
Epoch 81 Train Loss 3.66844856577185 Test Loss 6.411572486289034
Epoch 82 Train Loss 3.638395358799423 Test Loss 6.4516991251480444
Epoch 83 Train Loss 3.6074585124313 Test Loss 6.493267052126053
Epoch 84 Train Loss 3.575590275797446 Test Loss 6.534327974149266
Epoch 85 Train Loss 3.542709462712924 Test Loss 6.578973461829013
Epoch 86 Train Loss 3.50880811062333 Test Loss 6.6234288214287185
Epoch 87 Train Loss 3.473819043400098 Test Loss 6.670885682944814
Epoch 88 Train Loss 3.437709465918662 Test Loss 6.718973553273483
Epoch 89 Train Loss 3.4004367720378283 Test Loss 6.766311401059878
Epoch 90 Train Loss 3.3620418778955146 Test Loss 6.818577291563512
Epoch 91 Train Loss 3.3225882407831793 Test Loss 6.868845779621275
Epoch 92 Train Loss 3.2820777126206453 Test Loss 6.921508554324032
Epoch 93 Train Loss 3.2404697956203945 Test Loss 6.975030886357245
Epoch 94 Train Loss 3.197703433868599 Test Loss 7.0317418647107255
Epoch 95 Train Loss 3.153883670772145 Test Loss 7.088344236436367
Epoch 96 Train Loss 3.109039057602158 Test Loss 7.1470404048883545
Epoch 97 Train Loss 3.063174005013554 Test Loss 7.211930827025432
Epoch 98 Train Loss 3.016444513626651 Test Loss 7.269212739309723
Epoch 100 Train Loss 2.9206368323286798 Test Loss 7.3962402602133475
Epoch 101 Train Loss 2.8713566326384132 Test Loss 7.464667713726755
Epoch 102 Train Loss 2.821267464498629 Test Loss 7.530506740750247
Epoch 103 Train Loss 2.770776464879623 Test Loss 7.60051755782109
Epoch 104 Train Loss 2.719645467293592 Test Loss 7.671727675062527
Epoch 105 Train Loss 2.6675382443881857 Test Loss 7.742431848042028
Epoch 106 Train Loss 2.6145823259909755 Test Loss 7.8190208048849
Epoch 107 Train Loss 2.5612470681297146 Test Loss 7.892965630639561
Epoch 108 Train Loss 2.507483037724161 Test Loss 7.971774118310247
Epoch 109 Train Loss 2.4528662868454303 Test Loss 8.05247184264207
Epoch 110 Train Loss 2.397538020128886 Test Loss 8.134157422433242
Epoch 111 Train Loss 2.3418668253243804 Test Loss 8.221701781670522
Epoch 112 Train Loss 2.2859909973813415 Test Loss 8.303712071505169
Epoch 113 Train Loss 2.2298508449667334 Test Loss 8.398120942104947
Epoch 114 Train Loss 2.173402772656468 Test Loss 8.484378981040159
Epoch 115 Train Loss 2.117289170409113 Test Loss 8.579726143965344
Epoch 116 Train Loss 2.060902019117387 Test Loss 8.673114166943009
Epoch 117 Train Loss 2.0032721023341713 Test Loss 8.769643541729316
Epoch 118 Train Loss 1.9463021206927287 Test Loss 8.870287948512043
Epoch 119 Train Loss 1.8899192668745926 Test Loss 8.969245738988251
Epoch 120 Train Loss 1.8327150189275954 Test Loss 9.078556459259453
Epoch 121 Train Loss 1.7756970701283943 Test Loss 9.186027976585887
Epoch 122 Train Loss 1.7193678783554054 Test Loss 9.299503142593451
Epoch 123 Train Loss 1.6627993169815098 Test Loss 9.411756727275352
Epoch 124 Train Loss 1.6061979353772806 Test Loss 9.534411015956993
Epoch 125 Train Loss 1.5506762869570037 Test Loss 9.651121267369236
Epoch 126 Train Loss 1.4957417886062587 Test Loss 9.777567572921665
Epoch 127 Train Loss 1.440655126555286 Test Loss 9.902040002912251
Epoch 128 Train Loss 1.3861246529499223 Test Loss 10.035774981197287
Epoch 129 Train Loss 1.3331639979310579 Test Loss 10.15897413134875
Epoch 130 Train Loss 1.2802940586783715 Test Loss 10.294886072725264
Epoch 131 Train Loss 1.2275839949037501 Test Loss 10.43289317471636
Epoch 132 Train Loss 1.177406935826803 Test Loss 10.569099689924863
Epoch 133 Train Loss 1.1270208691037016 Test Loss 10.707231100006867
Epoch 134 Train Loss 1.076120977530289 Test Loss 10.85958499897158
Epoch 135 Train Loss 1.0277739582259409 Test Loss 11.005080385107851
Epoch 136 Train Loss 0.9803831022071967 Test Loss 11.15482453367029
Epoch 137 Train Loss 0.9329304945727067 Test Loss 11.311596745551817
Epoch 138 Train Loss 0.8864301460018558 Test Loss 11.4705743119845
Epoch 139 Train Loss 0.8412149744125906 Test Loss 11.633415498815568
Epoch 140 Train Loss 0.7972235679453394 Test Loss 11.799833647064245
Epoch 141 Train Loss 0.7540932493215069 Test Loss 11.971728728522748
Epoch 142 Train Loss 0.7118720822436743 Test Loss 12.14432042933953
Epoch 143 Train Loss 0.6711705028094233 Test Loss 12.317763583332995
Epoch 144 Train Loss 0.6313696784176096 Test Loss 12.494099378877701
Epoch 145 Train Loss 0.5927489790888665 Test Loss 12.680536532174946
Epoch 146 Train Loss 0.5559643565422293 Test Loss 12.860211152640867
Epoch 147 Train Loss 0.5204653064246969 Test Loss 13.04861699668387
Epoch 148 Train Loss 0.4862428534537159 Test Loss 13.23708923949125
--- Training complete. Saving final artifacts ---
Final model saved to modular_addition_artifacts/final_trained_model.pt
Training history saved to modular_addition_artifacts/training_history.pt
{'text/html': '        <script type="text/javascript">\n        window.PlotlyConfig = {MathJaxConfig: \'local\'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}\n        </script>\n        <script type="module">import "https://cdn.plot.ly/plotly-3.1.1.min"</script>\n        '}
{'text/html': '<div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: \'local\'};</script>\n        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.1.1.min.js" integrity="sha256-HUEFyfiTnZJxCxur99FjbKYTvKSzwDaD3/x5TqHpFu4=" crossorigin="anonymous"></script>                <div id="1e9a2826-fb69-484b-87ab-dfc01a784c25" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("1e9a2826-fb69-484b-87ab-dfc01a784c25")) {                    Plotly.newPlot(                        "1e9a2826-fb69-484b-87ab-dfc01a784c25",                        [{"hovertemplate":"Color=train\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"train","orientation":"v","showlegend":true,"x":{"dtype":"i1","bdata":"AGQ="},"xaxis":"x","y":{"dtype":"f8","bdata":"06\\u002fHXRnxEkD6wfLXdl0HQA=="},"yaxis":"y","type":"scatter"},{"hovertemplate":"Color=test\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e","legendgroup":"1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"test","orientation":"v","showlegend":true,"x":{"dtype":"i1","bdata":"AGQ="},"xaxis":"x","y":{"dtype":"f8","bdata":"4GWymUDvEkBB5rsBwJUdQA=="},"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Epoch"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Loss"},"type":"log"},"legend":{"title":{"text":"Color"},"tracegroupgap":0},"title":{"text":"Training curve Modular Addition"},"updatemenus":[{"active":-1,"buttons":[{"args":[{"xaxis.type":"log"}],"args2":[{"xaxis.type":"linear"}],"label":"Log x-axis","method":"relayout"}],"type":"buttons","x":-0.1,"y":1.0},{"active":0,"buttons":[{"args":[{"yaxis.type":"log"}],"args2":[{"yaxis.type":"linear"}],"label":"Log y-axis","method":"relayout"}],"type":"buttons","x":-0.1,"y":0.85}]},                        {"responsive": true}                    ).then(function(){\n                            \nvar gd = document.getElementById(\'1e9a2826-fb69-484b-87ab-dfc01a784c25\');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === \'none\') {{\n            console.log([gd, \'removed!\']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest(\'#notebook-container\');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest(\'.output\');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };            </script>        </div>'}
